{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lilymarzano/LMSpatialDataFrames/blob/master/Neural_Network_Tutorial_and_Assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2da7f472",
      "metadata": {
        "id": "2da7f472"
      },
      "source": [
        "# **Neural Network Regression and Classification**\n",
        "\n",
        "---\n",
        "\n",
        "## **Overview**\n",
        "This tutorial provides a step-by-step guide to building and training **neural networks** for both **regression** and **classification** tasks.  \n",
        "You will learn how to implement models using **Scikit-learn** and **TensorFlow/Keras**, perform **hyperparameter tuning**, and interpret model performance using key evaluation metrics.\n",
        "\n",
        "---\n",
        "\n",
        "## **Learning Objectives**\n",
        "By the end of this tutorial, you will be able to:\n",
        "\n",
        "1. Implement neural networks for **regression** and **classification** tasks using Scikit-learn and TensorFlow.  \n",
        "2. Apply **hyperparameter optimization** (e.g., Random Search, Grid Search, or KerasTuner) to systematically improve model performance.  \n",
        "3. Select and justify appropriate **activation functions**, **architectures**, and **training configurations** for different machine learning problems.  \n",
        "4. Evaluate model performance using key metrics such as **RMSE**, **RÂ²**, and **classification accuracy**.\n",
        "\n",
        "---\n",
        "\n",
        "## **Tutorial Outline**\n",
        "This notebook is divided into two main parts:\n",
        "\n",
        "### **Part 1 â€” Regression Task: Predicting Crop Yield**\n",
        "- You will use an environmental dataset to predict **USDA county-level crop yields**.  \n",
        "- In *Assignment 3*, you trained tree-based models (e.g., Random Forest and XGBoost).  \n",
        "- Here, you will extend that work by building a **neural network regressor**, capable of modeling complex, nonlinear relationships between climate variables and crop productivity.\n",
        "\n",
        "### **Part 2 â€” Classification Task: Recognizing Handwritten Digits**\n",
        "- You will explore the **MNIST dataset**, which contains images of handwritten digits (0â€“9).  \n",
        "- This section demonstrates how neural networks perform **image recognition** and **multi-class classification** using **softmax activation** for probabilistic prediction.\n",
        "\n",
        "---\n",
        "\n",
        "## **Why Learn Both Tasks?**\n",
        "- **Regression** shows how neural networks predict **continuous outcomes** (e.g., yield).  \n",
        "- **Classification** illustrates how they categorize **discrete classes** (e.g., digits).  \n",
        "Mastering both prepares you to design neural architectures suited to a wide range of real-world applications â€” from climate-based yield forecasting to computer vision.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eaafbf6b",
      "metadata": {
        "id": "eaafbf6b"
      },
      "source": [
        "# **0. Import Necessary Packages**\n",
        "\n",
        "Before we begin, we need to import all the essential Python libraries used in this tutorial.  \n",
        "Importing packages at the top of your notebook ensures **clarity**, **reproducibility**, and **ease of debugging**.\n",
        "\n",
        "**Installation Notes**\n",
        "\n",
        "Before running this tutorial, ensure all required libraries are installed.  \n",
        "ðŸ’¡ Run these commands **only once per environment** (e.g., in Jupyter Notebook, VS Code, or a Conda environment).\n",
        "\n",
        "```bash\n",
        "# Install all core packages for this tutorial\n",
        "%pip install tensorflow keras_tuner\n",
        "\n",
        "# (Optional) Upgrade TensorFlow to the latest version if needed\n",
        "%pip install --upgrade --force-reinstall tensorflow\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3c67ad74",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3c67ad74",
        "outputId": "aa3146a1-8d4d-43af-d92b-0c2c730809fd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.12/dist-packages (2.20.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.3.1)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (25.9.23)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google_pasta>=0.1.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt_einsum>=2.3.2 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from tensorflow) (25.0)\n",
            "Requirement already satisfied: protobuf>=5.28.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (6.33.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.32.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from tensorflow) (80.9.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.2.0)\n",
            "Requirement already satisfied: typing_extensions>=3.6.6 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (4.15.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.0.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.76.0)\n",
            "Requirement already satisfied: tensorboard~=2.20.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.20.0)\n",
            "Requirement already satisfied: keras>=3.10.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.12.0)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.3.5)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.15.1)\n",
            "Requirement already satisfied: ml_dtypes<1.0.0,>=0.5.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.5.4)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from keras>=3.10.0->tensorflow) (14.2.0)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.12/dist-packages (from keras>=3.10.0->tensorflow) (0.1.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.12/dist-packages (from keras>=3.10.0->tensorflow) (0.18.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.11.12)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.20.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.20.0->tensorflow) (12.0.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.20.0->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.20.0->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from werkzeug>=1.0.1->tensorboard~=2.20.0->tensorflow) (3.0.3)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.10.0->tensorflow) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.10.0->tensorflow) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.10.0->tensorflow) (0.1.2)\n",
            "Collecting tensorflow\n",
            "  Using cached tensorflow-2.20.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.5 kB)\n",
            "Collecting absl-py>=1.0.0 (from tensorflow)\n",
            "  Using cached absl_py-2.3.1-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting astunparse>=1.6.0 (from tensorflow)\n",
            "  Using cached astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
            "Collecting flatbuffers>=24.3.25 (from tensorflow)\n",
            "  Using cached flatbuffers-25.9.23-py2.py3-none-any.whl.metadata (875 bytes)\n",
            "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 (from tensorflow)\n",
            "  Using cached gast-0.6.0-py3-none-any.whl.metadata (1.3 kB)\n",
            "Collecting google_pasta>=0.1.1 (from tensorflow)\n",
            "  Using cached google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
            "Collecting libclang>=13.0.0 (from tensorflow)\n",
            "  Using cached libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl.metadata (5.2 kB)\n",
            "Collecting opt_einsum>=2.3.2 (from tensorflow)\n",
            "  Using cached opt_einsum-3.4.0-py3-none-any.whl.metadata (6.3 kB)\n",
            "Collecting packaging (from tensorflow)\n",
            "  Using cached packaging-25.0-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting protobuf>=5.28.0 (from tensorflow)\n",
            "  Using cached protobuf-6.33.1-cp39-abi3-manylinux2014_x86_64.whl.metadata (593 bytes)\n",
            "Collecting requests<3,>=2.21.0 (from tensorflow)\n",
            "  Using cached requests-2.32.5-py3-none-any.whl.metadata (4.9 kB)\n",
            "Collecting setuptools (from tensorflow)\n",
            "  Using cached setuptools-80.9.0-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting six>=1.12.0 (from tensorflow)\n",
            "  Using cached six-1.17.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
            "Collecting termcolor>=1.1.0 (from tensorflow)\n",
            "  Using cached termcolor-3.2.0-py3-none-any.whl.metadata (6.4 kB)\n",
            "Collecting typing_extensions>=3.6.6 (from tensorflow)\n",
            "  Using cached typing_extensions-4.15.0-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting wrapt>=1.11.0 (from tensorflow)\n",
            "  Using cached wrapt-2.0.1-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (9.0 kB)\n",
            "Collecting grpcio<2.0,>=1.24.3 (from tensorflow)\n",
            "  Using cached grpcio-1.76.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (3.7 kB)\n",
            "Collecting tensorboard~=2.20.0 (from tensorflow)\n",
            "  Using cached tensorboard-2.20.0-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting keras>=3.10.0 (from tensorflow)\n",
            "  Using cached keras-3.12.0-py3-none-any.whl.metadata (5.9 kB)\n",
            "Collecting numpy>=1.26.0 (from tensorflow)\n",
            "  Using cached numpy-2.3.5-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (62 kB)\n",
            "Collecting h5py>=3.11.0 (from tensorflow)\n",
            "  Using cached h5py-3.15.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (3.0 kB)\n",
            "Collecting ml_dtypes<1.0.0,>=0.5.1 (from tensorflow)\n",
            "  Using cached ml_dtypes-0.5.4-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (8.9 kB)\n",
            "Collecting wheel<1.0,>=0.23.0 (from astunparse>=1.6.0->tensorflow)\n",
            "  Using cached wheel-0.45.1-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting rich (from keras>=3.10.0->tensorflow)\n",
            "  Using cached rich-14.2.0-py3-none-any.whl.metadata (18 kB)\n",
            "Collecting namex (from keras>=3.10.0->tensorflow)\n",
            "  Using cached namex-0.1.0-py3-none-any.whl.metadata (322 bytes)\n",
            "Collecting optree (from keras>=3.10.0->tensorflow)\n",
            "  Using cached optree-0.18.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (34 kB)\n",
            "Collecting charset_normalizer<4,>=2 (from requests<3,>=2.21.0->tensorflow)\n",
            "  Using cached charset_normalizer-3.4.4-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (37 kB)\n",
            "Collecting idna<4,>=2.5 (from requests<3,>=2.21.0->tensorflow)\n",
            "  Using cached idna-3.11-py3-none-any.whl.metadata (8.4 kB)\n",
            "Collecting urllib3<3,>=1.21.1 (from requests<3,>=2.21.0->tensorflow)\n",
            "  Using cached urllib3-2.5.0-py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting certifi>=2017.4.17 (from requests<3,>=2.21.0->tensorflow)\n",
            "  Using cached certifi-2025.11.12-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting markdown>=2.6.8 (from tensorboard~=2.20.0->tensorflow)\n",
            "  Using cached markdown-3.10-py3-none-any.whl.metadata (5.1 kB)\n",
            "Collecting pillow (from tensorboard~=2.20.0->tensorflow)\n",
            "  Using cached pillow-12.0.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (8.8 kB)\n",
            "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard~=2.20.0->tensorflow)\n",
            "  Using cached tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl.metadata (1.1 kB)\n",
            "Collecting werkzeug>=1.0.1 (from tensorboard~=2.20.0->tensorflow)\n",
            "  Using cached werkzeug-3.1.3-py3-none-any.whl.metadata (3.7 kB)\n",
            "Collecting MarkupSafe>=2.1.1 (from werkzeug>=1.0.1->tensorboard~=2.20.0->tensorflow)\n",
            "  Using cached markupsafe-3.0.3-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (2.7 kB)\n",
            "Collecting markdown-it-py>=2.2.0 (from rich->keras>=3.10.0->tensorflow)\n",
            "  Using cached markdown_it_py-4.0.0-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting pygments<3.0.0,>=2.13.0 (from rich->keras>=3.10.0->tensorflow)\n",
            "  Using cached pygments-2.19.2-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich->keras>=3.10.0->tensorflow)\n",
            "  Using cached mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
            "Using cached tensorflow-2.20.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (620.7 MB)\n",
            "Using cached absl_py-2.3.1-py3-none-any.whl (135 kB)\n",
            "Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
            "Using cached flatbuffers-25.9.23-py2.py3-none-any.whl (30 kB)\n",
            "Using cached gast-0.6.0-py3-none-any.whl (21 kB)\n",
            "Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
            "Using cached grpcio-1.76.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (6.6 MB)\n",
            "Using cached h5py-3.15.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (5.1 MB)\n",
            "Using cached keras-3.12.0-py3-none-any.whl (1.5 MB)\n",
            "Using cached libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl (24.5 MB)\n",
            "Using cached ml_dtypes-0.5.4-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (5.0 MB)\n",
            "Using cached numpy-2.3.5-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.6 MB)\n",
            "Using cached opt_einsum-3.4.0-py3-none-any.whl (71 kB)\n",
            "Using cached protobuf-6.33.1-cp39-abi3-manylinux2014_x86_64.whl (323 kB)\n",
            "Using cached requests-2.32.5-py3-none-any.whl (64 kB)\n",
            "Using cached six-1.17.0-py2.py3-none-any.whl (11 kB)\n",
            "Using cached tensorboard-2.20.0-py3-none-any.whl (5.5 MB)\n",
            "Using cached setuptools-80.9.0-py3-none-any.whl (1.2 MB)\n",
            "Using cached termcolor-3.2.0-py3-none-any.whl (7.7 kB)\n",
            "Using cached typing_extensions-4.15.0-py3-none-any.whl (44 kB)\n",
            "Using cached wrapt-2.0.1-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (121 kB)\n",
            "Using cached packaging-25.0-py3-none-any.whl (66 kB)\n",
            "Using cached certifi-2025.11.12-py3-none-any.whl (159 kB)\n",
            "Using cached charset_normalizer-3.4.4-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (153 kB)\n",
            "Using cached idna-3.11-py3-none-any.whl (71 kB)\n",
            "Using cached markdown-3.10-py3-none-any.whl (107 kB)\n",
            "Using cached tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl (6.6 MB)\n",
            "Using cached urllib3-2.5.0-py3-none-any.whl (129 kB)\n",
            "Using cached werkzeug-3.1.3-py3-none-any.whl (224 kB)\n",
            "Using cached wheel-0.45.1-py3-none-any.whl (72 kB)\n",
            "Using cached namex-0.1.0-py3-none-any.whl (5.9 kB)\n",
            "Using cached optree-0.18.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (408 kB)\n",
            "Using cached pillow-12.0.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (7.0 MB)\n",
            "Using cached rich-14.2.0-py3-none-any.whl (243 kB)\n",
            "Using cached markdown_it_py-4.0.0-py3-none-any.whl (87 kB)\n",
            "Using cached markupsafe-3.0.3-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (22 kB)\n",
            "Using cached pygments-2.19.2-py3-none-any.whl (1.2 MB)\n",
            "Using cached mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n"
          ]
        }
      ],
      "source": [
        "%pip install tensorflow\n",
        "%pip install --upgrade --force-reinstall tensorflow\n",
        "%pip install keras_tuner"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "MCJ5KqUINFoj"
      },
      "id": "MCJ5KqUINFoj"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1f2dc97f",
      "metadata": {
        "id": "1f2dc97f"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# Core Python Libraries\n",
        "# ============================================================\n",
        "import os\n",
        "import warnings\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ============================================================\n",
        "# Scikit-learn (Machine Learning Utilities)\n",
        "# ============================================================\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "# ============================================================\n",
        "# TensorFlow / Keras (Deep Learning Framework)\n",
        "# ============================================================\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import Sequential, layers, models\n",
        "from tensorflow.keras.layers import Dense, Normalization, Dropout, BatchNormalization\n",
        "from tensorflow.keras.metrics import RootMeanSquaredError\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "# ============================================================\n",
        "# Keras Tuner (Hyperparameter Optimization)\n",
        "# ============================================================\n",
        "import keras_tuner as kt\n",
        "\n",
        "# ============================================================\n",
        "# Spatial Tools (for Mapping and Visualization)\n",
        "# ============================================================\n",
        "import geopandas as gpd\n",
        "\n",
        "# ============================================================\n",
        "# Warning Control & Random Seed (for Reproducibility)\n",
        "# ============================================================\n",
        "from sklearn.exceptions import ConvergenceWarning\n",
        "\n",
        "# Suppress non-critical warnings for cleaner output\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "SEED = 42\n",
        "tf.random.set_seed(SEED)\n",
        "\n",
        "print(f\"Libraries successfully loaded. Random seed set to {SEED}. TensorFlow version: {tf.__version__}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3ccc0fcd",
      "metadata": {
        "id": "3ccc0fcd"
      },
      "source": [
        "# **Part 1 â€” Regression: Predicting Crop Yield**\n",
        "\n",
        "This section demonstrates how to build and evaluate a **neural network regression model** to predict **USDA county-level crop yields** using climate and environmental features.  \n",
        "You will begin by exploring and preparing the dataset, defining the features and target variable, and finally splitting the data into training and testing subsets.\n",
        "\n",
        "---\n",
        "\n",
        "## **1.1 Load the Data and Perform a Quick Data Check**\n",
        "\n",
        "ðŸ’¡ **Tips**\n",
        "\n",
        "Loading your dataset correctly is the first step toward a clean and reproducible analysis.\n",
        "\n",
        "**Best Practices:**\n",
        "- Use `pd.read_csv()` to load tabular data.  \n",
        "- Specify **data types** for key identifier columns (e.g., `GEOID`, `STATEFP`) as `str` to preserve **leading zeros**, which are important for spatial joins and FIPS mapping.  \n",
        "- Perform quick data inspections to verify correct loading and structure.\n",
        "\n",
        "**Recommended checks:**\n",
        "- `df.shape` â†’ shows the number of rows (samples) and columns (features + target).  \n",
        "- `df.head()` â†’ previews the first few rows to confirm column names and formats.  \n",
        "- For large datasets, load only a few rows first using `nrows=5` to preview structure before reading the full file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3ad8d077",
      "metadata": {
        "id": "3ad8d077"
      },
      "outputs": [],
      "source": [
        "# Load dataset\n",
        "df = pd.read_csv(\"US_corn.csv\", dtype={\"GEOID\": str, \"STATEFP\": str})\n",
        "\n",
        "# Quick checks\n",
        "print(\"Data shape:\", df.shape)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "84f34c06",
      "metadata": {
        "id": "84f34c06"
      },
      "source": [
        "## **1.2 Define Features (X) and Target (y)**\n",
        "\n",
        "ðŸ’¡ **Tips:**  \n",
        "- In supervised machine learning, we separate the dataset into:  \n",
        "  - **Predictors (features)** â†’ the input variables the model will use.  \n",
        "  - **Target (label)** â†’ the variable we want to predict.  \n",
        "- Here:  \n",
        "  - The variable **`ln_yield`** is the **target** representing the natural log of crop yield (stabilizes variance and improves regression performance).  \n",
        "  - The selected **features** include climate indicators that influence yield outcomes.  \n",
        "- Define:  \n",
        "  - `X = df[features]` â†’ extracts only the chosen feature columns from the full DataFrame.  \n",
        "  - `y = df[\"ln_yield\"]` â†’ sets the target variable for prediction.  \n",
        "- Handle categorical variables:  \n",
        "  - `pd.get_dummies(X, columns=['STATEFP'])` converts the **state code** (`STATEFP`) from categorical to multiple binary (0/1) columns â€” one per state.  \n",
        "  - This process, known as **one-hot encoding**, lets models like Random Forest or XGBoost interpret categorical information numerically.  \n",
        "  - For tree-based models, keep all dummy columns; for linear models, use `drop_first=True` to avoid multicollinearity.  \n",
        "- Data validation checks:  \n",
        "  - `X.shape` and `y.shape` â†’ confirm that the number of rows matches between predictors and target.  \n",
        "  - `X.isna().any().any()` and `y.isna().any()` â†’ test for missing values in features and target.  \n",
        "  - If missing values exist:  \n",
        "    - For **features (`X`)**, impute using mean or median.  \n",
        "    - For **target (`y`)**, drop rows with missing labels to avoid training errors.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "76329f30",
      "metadata": {
        "id": "76329f30"
      },
      "outputs": [],
      "source": [
        "# Define feature set and target variable\n",
        "features = [\n",
        "    'STATEFP',\n",
        "    'sm',\n",
        "    'precip',\n",
        "    'tmean',\n",
        "    'vpdmean'\n",
        "]\n",
        "\n",
        "# Select features (X) and target (y)\n",
        "X = df[features]\n",
        "y = df[\"ln_yield\"]\n",
        "\n",
        "# Encode categorical variables\n",
        "X = pd.get_dummies(X, columns=['STATEFP'])\n",
        "\n",
        "# Quick data quality checks\n",
        "print(\"X shape:\", X.shape)\n",
        "print(\"y shape:\", y.shape)\n",
        "print(\"Any NA in X:\", X.isna().any().any())\n",
        "print(\"Any NA in y:\", y.isna().any())\n",
        "\n",
        "# If NA found in y, drop corresponding rows and then rerun X and y selection\n",
        "# df = df.dropna(subset=['ln_yield'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "78d198d3",
      "metadata": {
        "id": "78d198d3"
      },
      "source": [
        "## **1.3 Prepare Training and Test Data (Trainâ€“Test Split)**\n",
        "\n",
        "ðŸ’¡ **Tips:**  \n",
        "- A **trainâ€“test split** divides the dataset into two parts to evaluate how well a model generalizes to unseen data:  \n",
        "  - **Training set (70%)** â†’ used by the model to learn patterns and relationships between features (`X`) and the target (`y`).  \n",
        "  - **Test set (30%)** â†’ held out and only used for the final evaluation of model performance.  \n",
        "- The Scikit-Learn function `train_test_split()` automatically partitions your data:  \n",
        "  - `test_size=0.30` â†’ reserves 30% of the data for testing (you can adjust this depending on dataset size).  \n",
        "  - `random_state=42` â†’ sets a **random seed** to make the split **reproducible**, ensuring consistent results every time you run the code.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "81de74d6",
      "metadata": {
        "id": "81de74d6"
      },
      "outputs": [],
      "source": [
        "# Split data into training (70%) and testing (30%) sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Display the resulting dataset shapes\n",
        "print(\"Training set shape:\", X_train.shape)\n",
        "print(\"Testing set shape: \", X_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1ba79c67",
      "metadata": {
        "id": "1ba79c67"
      },
      "source": [
        "## **1.4 Neural Network Modeling: Using Scikit-learn**\n",
        "\n",
        "### **1.4.1 Baseline Neural Network**\n",
        "\n",
        "Before tuning or building more complex networks, itâ€™s essential to first create a **baseline model**.  \n",
        "A baseline provides a benchmark against which later improvements can be measured.\n",
        "\n",
        "In this section, weâ€™ll use Scikit-learnâ€™s `MLPRegressor` (Multilayer Perceptron) to fit a simple **feed-forward neural network** for crop yield prediction.\n",
        "\n",
        "ðŸ’¡ **Tips**\n",
        "\n",
        "- **Start simple:**  \n",
        "  Begin with a small network to establish a baseline performance before tuning.  \n",
        "\n",
        "- **Scale features:**  \n",
        "  Neural networks are sensitive to feature magnitudes.  \n",
        "  Use `StandardScaler()` to standardize features, which helps stabilize and accelerate convergence.  \n",
        "\n",
        "- **Model architecture:**  \n",
        "  A model with one hidden layer of 10 neurons (`hidden_layer_sizes=(10,)`) is a good minimal starting point.  \n",
        "\n",
        "- **Use a Pipeline:**  \n",
        "  Combine preprocessing and modeling steps using Scikit-learnâ€™s `Pipeline`.  \n",
        "  This approach prevents data leakage and ensures consistent transformations across training and prediction.  \n",
        "\n",
        "- **Evaluate performance:**  \n",
        "  Use both **RMSE (Root Mean Squared Error)** â€” to measure prediction error in target units â€” and **RÂ² (Coefficient of Determination)** â€” to evaluate how well the model explains variance in the data.  \n",
        "\n",
        "- **Convergence warnings:**  \n",
        "  If you encounter convergence warnings, increase `max_iter` (e.g., to `1000`).  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a3f76ef3",
      "metadata": {
        "id": "a3f76ef3"
      },
      "outputs": [],
      "source": [
        "# Build a pipeline: scaling + simple MLP (1 hidden layer with 10 neurons)\n",
        "nn_base = Pipeline([\n",
        "    (\"scaler\", StandardScaler()),\n",
        "    (\"mlp\", MLPRegressor(\n",
        "        hidden_layer_sizes=(10,),  # one hidden layer with 10 neurons\n",
        "        random_state=42,\n",
        "        max_iter=1000              # increase if you see convergence warnings\n",
        "    ))\n",
        "])\n",
        "\n",
        "# Fit the baseline model\n",
        "nn_base.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = nn_base.predict(X_test)\n",
        "\n",
        "# Evaluate model performance\n",
        "rmse_base = np.sqrt(mean_squared_error(y_test, y_pred)) # Calculate RMSE by taking the square root of MSE\n",
        "r2_base = r2_score(y_test, y_pred)\n",
        "\n",
        "print(\"Baseline Evaluation:\")\n",
        "print(f\"RMSE: {rmse_base:.4f}\")\n",
        "print(f\"RÂ²  : {r2_base:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fa9bd8bd",
      "metadata": {
        "id": "fa9bd8bd"
      },
      "source": [
        "### **1.4.2 Define Hyperparameter Search Space**\n",
        "\n",
        "ðŸ’¡ **Tips**\n",
        "\n",
        "- **Why tune?**  \n",
        "  Default parameters are rarely optimal â€” tuning helps balance **underfitting** (too simple) and **overfitting** (too complex).\n",
        "\n",
        "- **Pipeline parameters:**  \n",
        "  When tuning a model inside a `Pipeline`, prefix parameter names with the pipeline step, e.g., `\"mlp__hidden_layer_sizes\"`.\n",
        "\n",
        "- **Common hyperparameters:**\n",
        "  - `hidden_layer_sizes`: defines the number of neurons and layers (e.g., `(64, 32)` â†’ 2 hidden layers).  \n",
        "  - `activation`: nonlinear function that introduces flexibility (`'relu'`, `'tanh'`).  \n",
        "  - `alpha`: L2 regularization term that penalizes large weights to prevent overfitting.  \n",
        "  - `learning_rate_init`: initial step size for weight updates.  \n",
        "  - `early_stopping`: halts training automatically if validation performance stops improving.  \n",
        "  - `validation_fraction`: fraction of the training data reserved for validation when `early_stopping=True`.  \n",
        "    - By default, 10% (`0.1`) of the training data is held out for validation.  \n",
        "    - The model monitors validation loss and stops early when no improvement is seen for several epochs.  \n",
        "    - This reduces overfitting and saves computation time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f0c2dd41",
      "metadata": {
        "id": "f0c2dd41"
      },
      "outputs": [],
      "source": [
        "param_dist = {\n",
        "    \"mlp__hidden_layer_sizes\": [(32,), (64,), (64, 32), (128, 64, 32)],\n",
        "    \"mlp__activation\": [\"relu\", \"tanh\"],\n",
        "    \"mlp__alpha\": [1e-4, 1e-3, 1e-2],        # L2 regularization strength\n",
        "    \"mlp__learning_rate_init\": [1e-3, 1e-4],\n",
        "    \"mlp__early_stopping\": [True],\n",
        "    \"mlp__validation_fraction\": [0.1],\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c6b07fa3",
      "metadata": {
        "id": "c6b07fa3"
      },
      "source": [
        "### **1.4.3 Randomized Search with Cross-Validation**\n",
        "\n",
        "ðŸ’¡ **Tips**\n",
        "- **Why `RandomizedSearchCV`?**  \n",
        "  - Tests a *random subset* of parameter combinations instead of *every* combination (like Grid Search).  \n",
        "  - Much faster for large search spaces and still finds strong models.  \n",
        "- **Key arguments:**\n",
        "  - `n_iter=5`: tries 5 random parameter sets.  \n",
        "  - `cv=5`: uses **5-fold cross-validation** â†’ trains on 4 folds, validates on 1, repeats 5Ã— for more reliable performance.  \n",
        "  - `scoring=\"neg_mean_squared_error\"`: minimizes RMSE (Scikit-learn flips the sign because higher = better).  \n",
        "  - `n_jobs=-1`: uses all CPU cores for faster training.  \n",
        "  - `verbose=1`: prints progress to monitor the search.  \n",
        "- **Outputs to check:**\n",
        "  - `best_params_`: hyperparameters of the best model.  \n",
        "  - `best_score_`: mean CV performance (take square root of negative value for RMSE)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "99160449",
      "metadata": {
        "scrolled": true,
        "id": "99160449"
      },
      "outputs": [],
      "source": [
        "# Initialize randomized search\n",
        "random_search = RandomizedSearchCV(\n",
        "    estimator=nn_base,                   # pipeline defined earlier\n",
        "    param_distributions=param_dist,\n",
        "    n_iter=5,                           # number of random combinations to try\n",
        "    scoring=\"neg_mean_squared_error\",    # optimize for RMSE\n",
        "    cv=5,\n",
        "    verbose=1,\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# Run search on training data\n",
        "random_search.fit(X_train, y_train)\n",
        "\n",
        "# Print results\n",
        "best_params = random_search.best_params_\n",
        "best_cv_rmse = np.sqrt(-random_search.best_score_)\n",
        "\n",
        "print(\"Best Parameters:\", best_params)\n",
        "print(f\"Best Cross-Validation RMSE: {best_cv_rmse:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "61840126",
      "metadata": {
        "id": "61840126"
      },
      "source": [
        "### **1.4.4 Refit Best Model and Evaluate on Test Set**\n",
        "\n",
        "ðŸ’¡ **Tips**\n",
        "- After hyperparameter tuning, the **best model** is stored in `random_search.best_estimator_`.  \n",
        "- By default, Scikit-learn refits this best model on the **entire training set**, so itâ€™s ready for prediction.  \n",
        "- If you used `refit=False` in `RandomizedSearchCV`, call `.fit()` manually before prediction.  \n",
        "- Evaluate model generalization on the **held-out test set** using:\n",
        "  - **RMSE (Root Mean Squared Error)** â†’ lower values mean better accuracy.  \n",
        "  - **RÂ² (Coefficient of Determination)** â†’ closer to 1 indicates better variance explanation.  \n",
        "- Always compare against your **baseline model**:\n",
        "  - â†“ RMSE and â†‘ RÂ² â†’ tuning improved performance.  \n",
        "  - Small improvement â†’ baseline was already strong.  \n",
        "- **Visual check:**  \n",
        "  - Plot *Observed vs. Predicted* â€” perfect predictions fall along the red dashed **1:1 line**.  \n",
        "  - Patterns or scatter reveal bias and variance.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "325c612e",
      "metadata": {
        "id": "325c612e"
      },
      "outputs": [],
      "source": [
        "# Get best model from RandomizedSearchCV\n",
        "best_model = random_search.best_estimator_\n",
        "\n",
        "# Predictions on the test set\n",
        "y_pred_tuned = best_model.predict(X_test)\n",
        "\n",
        "# Evaluate tuned model\n",
        "rmse_tuned = np.sqrt(mean_squared_error(y_test, y_pred_tuned)) # Calculate RMSE by taking the square root of MSE\n",
        "r2_tuned = r2_score(y_test, y_pred_tuned)\n",
        "\n",
        "# Compare with baseline model (make sure you ran Section 4 first)\n",
        "print(\" Model Comparison (Test Set)\")\n",
        "print(f\"Baseline NN â†’ RMSE: {rmse_base:.4f},  RÂ²: {r2_base:.4f}\")\n",
        "print(f\"Tuned NN    â†’ RMSE: {rmse_tuned:.4f},  RÂ²: {r2_tuned:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8140deb1",
      "metadata": {
        "id": "8140deb1"
      },
      "outputs": [],
      "source": [
        "# Optional: Visualize Observed vs Predicted\n",
        "plt.figure(figsize=(5,4))\n",
        "plt.scatter(y_test, y_pred_tuned, alpha=0.6, edgecolor='none')\n",
        "lims = [min(y_test.min(), y_pred_tuned.min()), max(y_test.max(), y_pred_tuned.max())]\n",
        "plt.plot(lims, lims, 'r--', linewidth=1.2, label=\"Perfect prediction (1:1 line)\")\n",
        "plt.xlabel(\"Observed ln(yield)\", fontsize = 14)\n",
        "plt.ylabel(\"Predicted ln(yield)\", fontsize = 14)\n",
        "plt.title(\"Observed vs Predicted (Tuned Neural Network)\", fontsize = 14)\n",
        "plt.legend()\n",
        "plt.grid(alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "11bbc3bd",
      "metadata": {
        "id": "11bbc3bd"
      },
      "source": [
        "## **1.5 Neural Network Modeling: Using TensorFlow**\n",
        "\n",
        "### **1.5.1 Baseline Neural Network**\n",
        "\n",
        "In this section, youâ€™ll build a **simple feed-forward neural network** using **TensorFlow/Keras** for regression.  \n",
        "This model establishes a **baseline performance** that you can later improve with additional layers, regularization, or hyperparameter tuning.\n",
        "\n",
        "ðŸ’¡ **Tips**\n",
        "\n",
        "- **Start simple:**  \n",
        "  Begin with one or two dense layers to create a straightforward baseline model.  \n",
        "  Simplicity at this stage makes it easier to interpret performance gains after tuning.\n",
        "\n",
        "- **Normalize inputs:**  \n",
        "  Use Kerasâ€™s `Normalization()` layer to standardize feature values (mean = 0, variance = 1).  \n",
        "  This improves numerical stability and convergence speed during training.\n",
        "\n",
        "- **Build a Sequential model:**  \n",
        "  Stack layers in order:  \n",
        "  1. **Normalization layer** â†’ standardizes inputs.  \n",
        "  2. **Dense (hidden) layers** â†’ learn nonlinear relationships between features.  \n",
        "  3. **Dense (output) layer** â†’ produces continuous yield predictions.\n",
        "\n",
        "- **Compile the model:**  \n",
        "  - **Optimizer:** `'adam'` â€” adaptive learning rate; a good default for most problems.  \n",
        "  - **Loss:** `'mse'` â€” Mean Squared Error, standard for regression tasks.  \n",
        "  - **Metrics:** `RootMeanSquaredError()` â€” interpretable in the same units as the target variable.\n",
        "\n",
        "- **Train the model:**  \n",
        "  Use `epochs=100` initially to give the optimizer enough time to converge.  \n",
        "  Monitor the loss trend to determine if additional training or early stopping is needed.\n",
        "\n",
        "- **Evaluate results:**  \n",
        "  Compare predicted and actual yields using **RMSE** and **RÂ²** to assess model accuracy and goodness of fit."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "612bfd49",
      "metadata": {
        "scrolled": true,
        "id": "612bfd49"
      },
      "outputs": [],
      "source": [
        "# 1. Normalize input features (fit on training data only)\n",
        "normalizer = Normalization()\n",
        "normalizer.adapt(X_train.values)  # learns mean & variance from training features\n",
        "\n",
        "# 2. Define a simple Sequential model\n",
        "model = Sequential([\n",
        "    normalizer,                           # normalization layer\n",
        "    Dense(units=25, activation='relu'),   # hidden layer (25 neurons, ReLU activation)\n",
        "    Dense(units=15, activation='sigmoid'),# second hidden layer (15 neurons, Sigmoid activation)\n",
        "    Dense(units=1, activation='linear')   # output layer for regression\n",
        "])\n",
        "\n",
        "# 3. Compile the model\n",
        "model.compile(\n",
        "    optimizer='adam',                     # adaptive learning rate optimizer\n",
        "    loss='mse',                           # mean squared error loss\n",
        "    metrics=[RootMeanSquaredError()]      # RMSE for interpretability\n",
        ")\n",
        "\n",
        "# 4. Train the baseline model\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    epochs=100,\n",
        "    verbose=1                             # display training progress\n",
        ")\n",
        "\n",
        "# 5. Predict on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# 6. Evaluate baseline performance\n",
        "rmse_base = np.sqrt(mean_squared_error(y_test, y_pred)) # Calculate RMSE by taking the square root of MSE\n",
        "r2_base = r2_score(y_test, y_pred)\n",
        "\n",
        "print(\"\\nBaseline Neural Network Evaluation\")\n",
        "print(f\"RMSE: {rmse_base:.4f}\")\n",
        "print(f\"RÂ²  : {r2_base:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bb9a97cb",
      "metadata": {
        "id": "bb9a97cb"
      },
      "outputs": [],
      "source": [
        "## Model Summary and Interpretation\n",
        "## After training the baseline TensorFlow/Keras model, we can examine its architecture using the command:\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a40dff48",
      "metadata": {
        "id": "a40dff48"
      },
      "source": [
        "### **Model Summary Interpretation**\n",
        "\n",
        "The table above summarizes the structure and parameter count of your **baseline TensorFlow neural network**.\n",
        "\n",
        "| Component | Description | Interpretation |\n",
        "|------------|--------------|----------------|\n",
        "| **Model:** `\"sequential\"` | Indicates the model is a **Sequential** Keras model, meaning layers are stacked in order â€” each feeding into the next. | Simplifies design; ideal for straightforward feed-forward networks. |\n",
        "| **`normalization_1 (Normalization)`** | Standardizes inputs using the mean and variance learned during `normalizer.adapt(X_train)`. | Ensures all features are on a similar scale, helping the network converge faster and more stably. |\n",
        "| **`dense` (25 units, ReLU)** | Fully connected layer with 25 neurons using the ReLU activation function. | Learns nonlinear relationships among input features. |\n",
        "| **`dense_1` (15 units, Sigmoid)** | Second hidden layer with 15 neurons using the sigmoid activation. | Captures finer nonlinearities and introduces a smooth, bounded activation for complex interactions. |\n",
        "| **`dense_2` (1 unit, Linear)** | Output layer with one neuron and linear activation. | Produces continuous numeric output â€” appropriate for regression tasks. |\n",
        "\n",
        "---\n",
        "\n",
        "### **Parameter Breakdown**\n",
        "\n",
        "- **Normalization layer:** `33` parameters  \n",
        "  â†’ These are *non-trainable statistics* (mean and variance) used to normalize input features.  \n",
        "- **Dense(25):** `16 Ã— 25 + 25 = 425` parameters  \n",
        "  â†’ Each neuron has 16 input weights (one per input feature) plus a bias term.  \n",
        "- **Dense(15):** `25 Ã— 15 + 15 = 390` parameters  \n",
        "  â†’ Connects all 25 outputs from the previous layer to 15 neurons, plus biases.  \n",
        "- **Dense(1):** `15 Ã— 1 + 1 = 16` parameters  \n",
        "  â†’ Maps 15 features to one final continuous prediction.  \n",
        "\n",
        "---\n",
        "\n",
        "### **Totals**\n",
        "- **Total parameters:** 864  \n",
        "- **Trainable parameters:** 831 (weights and biases learned during training).  \n",
        "- **Non-trainable parameters:** 33 (mean and variance values from the Normalization layer).  "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c0fed64b",
      "metadata": {
        "id": "c0fed64b"
      },
      "source": [
        "### **1.5.2 Early Stopping**\n",
        "\n",
        "Training deep learning models for too many epochs can lead to **overfitting**, where the model performs well on training data but poorly on unseen data.  \n",
        "To prevent this, we use **EarlyStopping**, a Keras callback that halts training when performance on a validation set stops improving.\n",
        "\n",
        "---\n",
        "\n",
        "ðŸ’¡ **Tips**\n",
        "\n",
        "- **Purpose:** Early stopping monitors a chosen metric (e.g., validation loss) and **stops training automatically** when improvement stalls.  \n",
        "- **Key arguments:**\n",
        "  - **`monitor='val_loss'`** â†’ tracks validation loss after each epoch.  \n",
        "  - **`patience=5`** â†’ waits for 5 epochs of no improvement before stopping (acts as a tolerance buffer).  \n",
        "  - **`restore_best_weights=True`** â†’ reloads the model weights from the epoch with the best validation loss (prevents using overfitted weights).  \n",
        "  - **`verbose=1`** â†’ prints a message when training stops early.  \n",
        "- **Validation data:**\n",
        "  - `validation_split=0.2` â†’ automatically reserves 20% of the training data for validation, or  \n",
        "  - A separate validation dataset (`X_val`, `y_val`).  Use `validation_split=0.2` (or provide a separate validation set) so the callback has data to monitor.  \n",
        "- **Epochs setting:**\n",
        "  - You can set a high number of epochs (e.g., 100â€“200). EarlyStopping will typically stop training much earlier once convergence is reached.  \n",
        "  - This approach avoids manual tuning of epoch counts.\n",
        "- **Batch Size:**\n",
        "    The `batch_size` parameter controls how many training samples are processed before the modelâ€™s internal weights are updated.\n",
        "\n",
        "    - **Typical values:** 16, 32, 64, or 128 â€” depending on dataset size and hardware.  \n",
        "    - **Smaller batches:** More frequent updates â†’ better generalization but slower training.  \n",
        "    - **Larger batches:** Faster per epoch but may underfit or converge to suboptimal minima.  \n",
        "    - **Recommendation:** Start with `batch_size=32` (a balanced default) and adjust based on model stability or training speed.\n",
        "\n",
        "\n",
        "**Rationale**\n",
        "\n",
        "Early stopping provides several benefits:\n",
        "- **Prevents overfitting:** Stops once the validation error begins to increase.  \n",
        "- **Saves time:** Avoids unnecessary training beyond the optimal point.  \n",
        "- **Improves generalization:** Retains the best model observed during training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "12acb836",
      "metadata": {
        "scrolled": true,
        "id": "12acb836"
      },
      "outputs": [],
      "source": [
        "# 1. Define the EarlyStopping callback\n",
        "earlystop = EarlyStopping(\n",
        "    monitor='val_loss',          # metric to monitor\n",
        "    patience=5,                  # epochs to wait before stopping\n",
        "    restore_best_weights=True,   # revert to best model weights\n",
        "    verbose=1                    # print when training stops early\n",
        ")\n",
        "\n",
        "# 2. Train the model with a validation split\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    validation_split=0.2,        # 20% of training data used for validation\n",
        "    epochs=100,                  # high limit; EarlyStopping will stop earlier\n",
        "    batch_size=32,\n",
        "    callbacks=[earlystop],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# 3. Predict on test data\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# 4. Evaluate model performance\n",
        "rmse_earlystop = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "r2_earlystop = r2_score(y_test, y_pred)\n",
        "\n",
        "\n",
        "# 5. Compare with baseline model without early stopping\n",
        "print(\"\\n Model Performance on Test Set (with Early Stopping):\")\n",
        "print(f\"RMSE : {rmse_earlystop:.4f}\")\n",
        "print(f\"RÂ²   : {r2_earlystop:.4f}\")\n",
        "\n",
        "print(\"\\n Comparison with Baseline Model:\")\n",
        "print(f\"Baseline RMSE : {rmse_base:.4f}\")\n",
        "print(f\"Baseline RÂ²   : {r2_base:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "83c3a79c",
      "metadata": {
        "id": "83c3a79c"
      },
      "source": [
        "### **1.5.3 Building a Tunable Neural Network with KerasTuner**\n",
        "\n",
        "ðŸ’¡ **Tips**\n",
        "\n",
        "- **Core Function:**  \n",
        "    Use **KerasTuner** to automatically explore and evaluate different **hyperparameter combinations** to find the best-performing neural network configuration that minimizes **validation error** (e.g., RMSE).  \n",
        "\n",
        "    This model, built with the **Sequential API**, performs regression on normalized features while tuning key parameters such as:\n",
        "    - Number of neurons per layer,  \n",
        "    - Activation functions, and  \n",
        "    - Learning rate.\n",
        "\n",
        "- **Search Process:**  \n",
        "    KerasTuner automates hyperparameter tuning by:\n",
        "    1. Building, training, and evaluating multiple model configurations with different hyperparameter values.  \n",
        "    2. Selecting the configuration that achieves the **lowest validation loss** (MSE or RMSE).  \n",
        "    3. Reporting the optimal hyperparameters for retraining on the full training set.\n",
        "\n",
        "- **Key Hyperparameters to Explore:**\n",
        "\n",
        "| **Hyperparameter** | **Description** | **Effect** |\n",
        "|--------------------|----------------|-------------|\n",
        "| **`units`** | Number of neurons in each hidden layer (controls model capacity). | Too few â†’ underfitting; Too many â†’ overfitting or slower training. |\n",
        "| **`activation`** | Nonlinear transformation applied to layer outputs (`'relu'`, `'tanh'`, `'sigmoid'`). | Enables the network to learn complex, nonlinear feature relationships. |\n",
        "| **`learning_rate`** | Step size for optimizer updates (`1e-2`, `1e-3`, `1e-4`). | Smaller â†’ stable convergence; Larger â†’ faster training but may overshoot minima. |\n",
        "| **`dropout`** *(optional)* | Fraction of neurons randomly dropped during training (0.0â€“0.4). | Reduces overfitting and improves generalization. |\n",
        "\n",
        "- **Model Architecture (Sequential API):**\n",
        "  1. **Normalization Layer:**  \n",
        "     - Standardizes input features using a pre-adapted normalizer (`normalizer.adapt(X_train)` was executed earlier).  \n",
        "     - Ensures that all features contribute evenly during training.  \n",
        "  2. **Hidden Layers:**  \n",
        "     - Two fully connected (`Dense`) layers with tunable `units` and `activation`.  \n",
        "     - Capture complex nonlinear relationships between input features.  \n",
        "  3. **Output Layer:**  \n",
        "     - A single neuron with **linear activation**, suitable for continuous-valued regression outputs.  \n",
        "  4. **Optimizer:**  \n",
        "     - Uses the **Adam** optimizer with a tunable `learning_rate` to balance training stability and convergence speed.\n",
        "\n",
        "- **Key Keras Functions:**\n",
        "  - **`model = keras.Sequential()`** â†’ Initializes a linear stack of layers where each layer feeds directly into the next.  \n",
        "  - **`model.add(layer)`** â†’ Appends a new layer to the model in sequential order.  \n",
        "  - **`model.compile()`** â†’ Configures the model for training by specifying:  \n",
        "    - **`optimizer`** â†’ The algorithm for updating weights (e.g., Adam).  \n",
        "    - **`loss`** â†’ The objective function to minimize (e.g., MSE for regression).  \n",
        "    - **`metrics`** â†’ Additional metrics to monitor (e.g., RMSE).\n",
        "\n",
        "- **Outcome:**  \n",
        "  - After tuning, KerasTuner reports the **best hyperparameter configuration** (e.g., number of units, activation function, and learning rate) that yields the **lowest validation RMSE**.  \n",
        "  - You can then rebuild and retrain the model using these optimized settings to achieve the best regression performance.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6fa5aae6",
      "metadata": {
        "id": "6fa5aae6"
      },
      "outputs": [],
      "source": [
        "# The 'normalizer' has already been adapted on X_train\n",
        "def build_model(hp):\n",
        "    \"\"\"\n",
        "    KerasTuner-compatible model builder using the Sequential API.\n",
        "    Each hidden layer can have its own 'units' and 'activation'.\n",
        "    \"\"\"\n",
        "    model = keras.Sequential(name=\"regression_model\")\n",
        "\n",
        "    # Input normalization (use pre-adapted normalizer)\n",
        "    model.add(normalizer)\n",
        "\n",
        "    # ===== Layer 1 hyperparameters =====\n",
        "    units_1 = hp.Int(\"units_1\", min_value=16, max_value=128, step=16)\n",
        "    activation_1 = hp.Choice(\"activation_1\", [\"relu\", \"tanh\", \"sigmoid\"])\n",
        "\n",
        "    # ===== Layer 2 hyperparameters (independent) =====\n",
        "    units_2 = hp.Int(\"units_2\", min_value=8, max_value=128, step=8)\n",
        "    activation_2 = hp.Choice(\"activation_2\", [\"relu\", \"tanh\", \"sigmoid\"])\n",
        "\n",
        "    # Hidden layers (apply each to the previous tensor using Sequential.add)\n",
        "    model.add(layers.Dense(units_1, activation=activation_1, name=\"hidden_1\"))\n",
        "    model.add(layers.Dense(units_2, activation=activation_2, name=\"hidden_2\"))\n",
        "\n",
        "    # Output layer for regression (single numeric value)\n",
        "    model.add(layers.Dense(1, activation=\"linear\", name=\"output\"))\n",
        "\n",
        "    # Tunable learning rate for Adam optimizer\n",
        "    lr = hp.Choice(\"learning_rate\", [1e-2, 1e-3, 1e-4])\n",
        "\n",
        "    # Compile the model\n",
        "    model.compile(\n",
        "        optimizer=keras.optimizers.Adam(learning_rate=lr),\n",
        "        loss=\"mse\",\n",
        "        metrics=[RootMeanSquaredError(name=\"rmse\")]\n",
        "    )\n",
        "\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "154ffebc",
      "metadata": {
        "id": "154ffebc"
      },
      "source": [
        "### **1.5.4 Find the Best Hyperparameters Using Random Search**\n",
        "\n",
        "ðŸ’¡ **Tips**\n",
        "\n",
        "- **Purpose:**  \n",
        "  - Automatically explore multiple combinations of hyperparameters using **KerasTunerâ€™s Random Search** to identify the configuration that minimizes the **validation RMSE (Root Mean Squared Error)**.  \n",
        "  - This step replaces manual trial-and-error with an efficient, systematic search across a defined hyperparameter space.\n",
        "\n",
        "- **Initialize the tuner:**  \n",
        "  - Use `kt.RandomSearch()` to automatically explore a defined number of random hyperparameter combinations.  \n",
        "  - Each trial trains a model with a unique set of hyperparameters.  \n",
        "  - The tuner tracks validation performance and records the configuration that minimizes the chosen metric (`val_rmse`).  \n",
        "  - Key parameters:\n",
        "    - **`objective`** â†’ Metric to optimize (`val_rmse` for regression, minimized here).  \n",
        "    - **`max_trials`** â†’ Number of random combinations to test (start small, e.g., 5â€“10).  \n",
        "    - **`directory` / `project_name`** â†’ Location to save tuning logs and model checkpoints.  \n",
        "    - **`seed`** â†’ Ensures reproducible tuning results.\n",
        "\n",
        "- **Define EarlyStopping:**  \n",
        "  - The `keras.callbacks.EarlyStopping()` callback monitors validation performance and halts training when improvements stop.  \n",
        "  - This prevents overfitting and saves computation time.  \n",
        "  - Key arguments:\n",
        "    - **`monitor='val_rmse'`** â†’ Tracks validation RMSE.  \n",
        "    - **`patience=5`** â†’ Stops after 5 epochs with no improvement.  \n",
        "    - **`mode='min'`** â†’ Minimizes the metric (lower RMSE = better).  \n",
        "    - **`restore_best_weights=True`** â†’ Automatically reverts to the best model weights.\n",
        "\n",
        "- **Run the tuning process:**  \n",
        "  - Use `tuner.search()` to start the hyperparameter optimization.  \n",
        "  - The tuner builds and trains multiple models using different hyperparameter sets.  \n",
        "  - Each trial:\n",
        "    - Trains the model for up to 100 epochs (usually fewer due to EarlyStopping).  \n",
        "    - Reserves 20% of training data for validation (`validation_split=0.2`).  \n",
        "  - Progress is printed to track each trialâ€™s performance.\n",
        "\n",
        "- **Retrieve the best hyperparameters:**  \n",
        "  - Once tuning completes, call `tuner.get_best_hyperparameters(num_trials=1)` to extract the best configuration.  \n",
        "  - Access it using `[0]` to get the top result and `.values` to view the hyperparameter dictionary.  \n",
        "  - These optimal parameters can be used to rebuild and retrain the model for final evaluation.\n",
        "\n",
        "**Summary:**  \n",
        "This workflow automates hyperparameter optimization with **Random Search**, while **EarlyStopping** ensures efficient training and prevents overfitting.  \n",
        "The result is a set of **best-performing hyperparameters** that minimize validation error and improve model generalization.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2ce4a943",
      "metadata": {
        "scrolled": true,
        "id": "2ce4a943"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# STEP 1 â€” Initialize the Random Search Tuner\n",
        "# ============================================================\n",
        "\n",
        "# KerasTuner RandomSearch explores random combinations of hyperparameters\n",
        "# to identify the model configuration that minimizes validation RMSE.\n",
        "tuner = kt.RandomSearch(\n",
        "    build_model,                               # model-building function\n",
        "    objective=kt.Objective('val_rmse', 'min'), # metric to minimize (validation RMSE)\n",
        "    max_trials=5,                              # number of random hyperparameter combinations to test\n",
        "    seed=42,                                   # ensures reproducibility\n",
        "    directory='kt_rand',                       # directory to store tuner results\n",
        "    project_name='crop_yield'                  # subfolder name for this tuning experiment\n",
        ")\n",
        "\n",
        "# ============================================================\n",
        "# STEP 2 â€” Define EarlyStopping Callback\n",
        "# ============================================================\n",
        "\n",
        "# EarlyStopping halts training automatically if validation RMSE stops improving,\n",
        "# which prevents overfitting and reduces computation time.\n",
        "early_stop = keras.callbacks.EarlyStopping(\n",
        "    monitor='val_rmse',        # metric to monitor\n",
        "    patience=5,                # stop after 5 epochs of no improvement\n",
        "    mode='min',                # minimize RMSE\n",
        "    restore_best_weights=True  # revert to the model weights with best validation RMSE\n",
        ")\n",
        "\n",
        "# ============================================================\n",
        "# STEP 3 â€” Run the Random Search Tuning Process\n",
        "# ============================================================\n",
        "\n",
        "# The tuner trains multiple models, each with a unique combination of hyperparameters.\n",
        "# Each trial uses a validation split to evaluate performance and rank results.\n",
        "tuner.search(\n",
        "    X_train, y_train,\n",
        "    epochs=100,                 # maximum epochs per trial; EarlyStopping typically halts earlier\n",
        "    validation_split=0.2,       # use 20% of training data for validation\n",
        "    callbacks=[early_stop],     # apply EarlyStopping during training\n",
        "    verbose=2                   # print progress for each trial\n",
        ")\n",
        "\n",
        "# ============================================================\n",
        "# STEP 4 â€” Retrieve the Best Hyperparameter Combination\n",
        "# ============================================================\n",
        "\n",
        "# Once tuning completes, extract the best hyperparameters based on validation RMSE.\n",
        "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
        "\n",
        "# Display the optimal hyperparameter configuration\n",
        "print(\"Best hyperparameters:\", best_hps.values)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cf72f926",
      "metadata": {
        "id": "cf72f926"
      },
      "source": [
        "### **1.5.5 Retrain and Evaluate the Best Model**\n",
        "\n",
        "ðŸ’¡ **Tips**\n",
        "\n",
        "- **Purpose:**  \n",
        "  Rebuild and train the model using the **best hyperparameters (`best_hps`)** found during tuning.  \n",
        "  Evaluate its final performance on the **held-out test set** to confirm how well the tuned model generalizes to unseen data.\n",
        "\n",
        "- **Build and Train:**  \n",
        "  - `tuner.hypermodel.build(best_hps)` reconstructs the model architecture using the optimal hyperparameter configuration.  \n",
        "  - Train the model on `(X_train, y_train)` using the same settings as in tuning:  \n",
        "    - **`validation_split=0.2`** â†’ reserves 20% of training data for validation.  \n",
        "    - **`callbacks=[early_stop]`** â†’ applies EarlyStopping to prevent overfitting.  \n",
        "  - Keeping these parameters consistent ensures comparable training behavior and performance.\n",
        "\n",
        "- **Predictions & Evaluation:**  \n",
        "  - Use `best_model.predict(X_test)` to generate predictions on unseen data.  \n",
        "  - Compute:  \n",
        "    - **RMSE** â†’ `mean_squared_error(y_test, y_pred, squared=False)` for average prediction error.  \n",
        "    - **RÂ²** â†’ `r2_score(y_test, y_pred)` for the proportion of explained variance.  \n",
        "  - Lower RMSE and higher RÂ² indicate better predictive accuracy and generalization.\n",
        "\n",
        "- **Visualization:**  \n",
        "  - Plot **Observed vs Predicted** values to assess model fit.  \n",
        "  - Perfect predictions align along the **red dashed 1:1 line**.  \n",
        "  - Deviations from the line highlight bias or variance patterns.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b624d345",
      "metadata": {
        "scrolled": true,
        "id": "b624d345"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# STEP 1 â€” Build the Best Model from Tuned Hyperparameters\n",
        "# ============================================================\n",
        "best_model = tuner.hypermodel.build(best_hps)\n",
        "\n",
        "# ============================================================\n",
        "# STEP 2 â€” Train the Model with Consistent Settings\n",
        "# ============================================================\n",
        "history = best_model.fit(\n",
        "    X_train, y_train,\n",
        "    epochs=100,                 # upper limit; EarlyStopping will likely stop earlier\n",
        "    validation_split=0.2,       # consistent with tuning\n",
        "    callbacks=[early_stop],     # reuse EarlyStopping for stable training\n",
        "    verbose=2\n",
        ")\n",
        "\n",
        "# ============================================================\n",
        "# STEP 3 â€” Evaluate Model Performance on Test Set\n",
        "# ============================================================\n",
        "y_pred_tuned_tf = best_model.predict(X_test).ravel()  # flatten predictions for metrics\n",
        "\n",
        "rmse_best = mean_squared_error(y_test, y_pred_tuned_tf, squared=False)\n",
        "r2_best   = r2_score(y_test, y_pred_tuned_tf)\n",
        "\n",
        "print(\"\\nModel Performance on Test Set (with hyperparameter tuning):\")\n",
        "print(f\"RMSE : {rmse_best:.4f}\")\n",
        "print(f\"RÂ²   : {r2_best:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "edd0da5d",
      "metadata": {
        "id": "edd0da5d"
      },
      "outputs": [],
      "source": [
        "# Visualize Observed vs Predicted\n",
        "plt.figure(figsize=(5,4))\n",
        "plt.scatter(y_test, y_pred_tuned_tf, alpha=0.6, edgecolor='none')\n",
        "lims = [min(y_test.min(), y_pred_tuned_tf.min()), max(y_test.max(), y_pred_tuned_tf.max())]\n",
        "plt.plot(lims, lims, 'r--', linewidth=1.2, label=\"Perfect prediction (1:1 line)\")\n",
        "plt.xlabel(\"Observed ln(yield)\", fontsize=14)\n",
        "plt.ylabel(\"Predicted ln(yield)\", fontsize=14)\n",
        "plt.title(\"Observed vs Predicted (Tuned Neural Network using TensorFlow)\", fontsize=14)\n",
        "plt.legend()\n",
        "plt.grid(alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bb0bfcb5",
      "metadata": {
        "id": "bb0bfcb5"
      },
      "source": [
        "## **1.6 Using the Trained Model to Predict Yield Changes under +1 Â°C Warming**\n",
        "\n",
        "This section applies the trained neural network model to estimate how county-level crop yields respond to a **+1 Â°C increase in mean temperature**, relative to the **2012â€“2017 average climate conditions**.  \n",
        "For detailed background on this climate-impact methodology, refer to *Assignment 3*.\n",
        "\n",
        "### **1.6.1 Generate County-Level Predicted Yield (2012â€“2017 Average)**\n",
        "\n",
        "ðŸ’¡ **Tips**\n",
        "\n",
        "- **Objective:**  \n",
        "  Estimate each countyâ€™s representative crop yield under average climate and management conditions from **2012â€“2017** using the trained model.\n",
        "\n",
        "- **Key Steps:**\n",
        "  1. **Aggregate climatology:**  \n",
        "     Collapse multiple years into county-level means to represent long-term average conditions:  \n",
        "     ```python\n",
        "     county_means = df.groupby([\"GEOID\", \"STATEFP\"])[features_withoutSTATEFP].mean().reset_index()\n",
        "     ```\n",
        "  2. **Preserve FIPS codes:**  \n",
        "     Convert `GEOID` and `STATEFP` to strings to retain leading zeros required for spatial joins.\n",
        "  3. **Prepare features:**  \n",
        "     Align columns to match the modelâ€™s input feature list and one-hot encode `STATEFP` for model compatibility.  \n",
        "  4. **Predict and back-transform:**  \n",
        "     - Model outputs are in **log-yield (ln_yield)**.  \n",
        "     - Apply `np.exp()` to convert predictions to original yield units.  \n",
        "  5. **Validate results:**  \n",
        "     Confirm that predicted yields fall within plausible crop-specific ranges."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0af94516",
      "metadata": {
        "id": "0af94516"
      },
      "outputs": [],
      "source": [
        "# --- Compute county-level mean climate features (2012â€“2017 average) ---\n",
        "features_withoutSTATEFP = [\"sm\", \"precip\", \"tmean\", \"vpdmean\"]\n",
        "\n",
        "county_means = (\n",
        "    df.groupby([\"GEOID\", \"STATEFP\"])[features_withoutSTATEFP]\n",
        "      .mean()\n",
        "      .reset_index()\n",
        ")\n",
        "\n",
        "# --- Prepare model input features ---\n",
        "X_county_means = county_means[features]\n",
        "\n",
        "# One-hot encode STATEFP for model compatibility\n",
        "X_county_means = pd.get_dummies(X_county_means, columns=[\"STATEFP\"])\n",
        "\n",
        "# --- Predict county-level yields ---\n",
        "county_means[\"pred_ln_yield\"] = best_model.predict(X_county_means)\n",
        "county_means[\"pred_yield\"] = np.exp(county_means[\"pred_ln_yield\"])  # back-transform\n",
        "\n",
        "county_means.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1b8a1cce",
      "metadata": {
        "id": "1b8a1cce"
      },
      "source": [
        "### **1.6.2 Generate Predicted County Yield under +1 Â°C Scenario**\n",
        "\n",
        "ðŸ’¡ **Tips**\n",
        "\n",
        "- **Objective:**  \n",
        "  Simulate how a **+1 Â°C increase in mean temperature (`tmean`)** affects county-level crop yields, assuming all other climate and management variables remain constant.  \n",
        "  This sensitivity experiment quantifies the potential impact of uniform warming on crop productivity.\n",
        "\n",
        "- **Key Steps:**\n",
        "  1. **Create the +1 Â°C scenario:**  \n",
        "     - Copy the baseline dataset (`X_county_means`) to `X_county_plus1C`.  \n",
        "     - Increase the `tmean` variable by **+1 Â°C** to represent the warming scenario.  \n",
        "\n",
        "  2. **Predict yields under the warming scenario:**  \n",
        "     - Use the trained model to generate **log-transformed yield predictions** (`pred_ln_yield_plus1C`).  \n",
        "     - Apply `np.exp()` to convert predictions back to the original yield units.  \n",
        "\n",
        "  3. **Compute yield change metrics:**  \n",
        "     - **Absolute change:**  \n",
        "       $\n",
        "       \\Delta Yield = Yield_{+1Â°C} - Yield_{Baseline}\n",
        "       $  \n",
        "     - **Percentage change:**  \n",
        "       $\n",
        "       \\%\\Delta Yield = \\frac{\\Delta Yield}{Yield_{Baseline}} \\times 100\n",
        "       $\n",
        "\n",
        "  4. **Interpretation:**  \n",
        "     - **Negative values** â†’ yield decline under warming (heat stress).  \n",
        "     - **Positive values** â†’ yield increase (possible benefit in cooler regions).  \n",
        "\n",
        "  5. **Output:**  \n",
        "     - Store all results in the same DataFrame (`county_means`) for subsequent spatial mapping and visualization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cd51e32b",
      "metadata": {
        "id": "cd51e32b"
      },
      "outputs": [],
      "source": [
        "# --- Create +1Â°C scenario ---\n",
        "X_county_plus1C = X_county_means.copy()\n",
        "X_county_plus1C[\"tmean\"] = X_county_plus1C[\"tmean\"] + 1  # add +1Â°C to mean temperature\n",
        "\n",
        "# --- Predict yields under the +1Â°C scenario ---\n",
        "county_means[\"pred_ln_yield_plus1C\"] = best_model.predict(X_county_plus1C)\n",
        "county_means[\"pred_yield_plus1C\"] = np.exp(county_means[\"pred_ln_yield_plus1C\"])  # back-transform to yield units\n",
        "\n",
        "# --- Compute yield differences (absolute and percentage) ---\n",
        "county_means[\"pred_yield_plus1C_dif\"] = (\n",
        "    county_means[\"pred_yield_plus1C\"] - county_means[\"pred_yield\"]\n",
        ")\n",
        "county_means[\"pred_yield_plus1C_dif_percent\"] = (\n",
        "    county_means[\"pred_yield_plus1C_dif\"] / county_means[\"pred_yield\"] * 100\n",
        ")\n",
        "\n",
        "# Preview results\n",
        "county_means.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3089ac71",
      "metadata": {
        "id": "3089ac71"
      },
      "source": [
        "### **1.6.3 Merge Geospatial Shapefile with Tabular Data**\n",
        "\n",
        "ðŸ’¡ **Tips**\n",
        "\n",
        "- **Objective:**  \n",
        "  Join county yield predictions with county geometries to enable mapping and spatial analysis.\n",
        "\n",
        "- **Key steps:**  \n",
        "  1. Ensure `GEOID` and `STATEFP` are strings in both `county_means` and shapefiles.  \n",
        "  2. Read county (`tl_2023_us_county.shp`) and state (`tl_2023_us_state.shp`) shapefiles.  \n",
        "  3. Align coordinate reference systems (CRS).  \n",
        "  4. Merge shapefile and tabular data using `GeoDataFrame.merge(on=[\"GEOID\",\"STATEFP\"])`.  \n",
        "  5. Keep only states present in your dataset for cleaner maps.  \n",
        "  6. Check merge results (`.isna().sum()`, row counts, CRS consistency)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "03d1d75f",
      "metadata": {
        "id": "03d1d75f"
      },
      "outputs": [],
      "source": [
        "# --- Filepaths ---\n",
        "url_counties = \"tl_2023_us_county/tl_2023_us_county.shp\"\n",
        "url_states = \"tl_2023_us_state/tl_2023_us_state.shp\"\n",
        "\n",
        "# --- Read shapefiles ---\n",
        "counties = gpd.read_file(url_counties)\n",
        "states = gpd.read_file(url_states)\n",
        "\n",
        "# --- Align CRS ---\n",
        "states = states.to_crs(counties.crs)\n",
        "\n",
        "# --- Merge yield data with county geometries ---\n",
        "counties_merged = counties.merge(county_means, on=[\"GEOID\", \"STATEFP\"])\n",
        "\n",
        "# --- Keep only states that have data ---\n",
        "state_fips_with_data = counties_merged[\"STATEFP\"].unique()\n",
        "states_with_data = states[states[\"STATEFP\"].isin(state_fips_with_data)]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a0e4a63d",
      "metadata": {
        "id": "a0e4a63d"
      },
      "source": [
        "### **1.6.4 Visualization â€” Predicted Yield Maps**\n",
        "\n",
        "ðŸ’¡ **Tips**\n",
        "\n",
        "- **Objective:**  \n",
        "  Visualize county-level predicted yields under three scenarios:\n",
        "  - Baseline (2012â€“2017 average)  \n",
        "  - +1 Â°C warming scenario  \n",
        "  - Yield change (+1 Â°C â€“ baseline)\n",
        "\n",
        "- **Map design:**  \n",
        "  - **Colormaps:**  \n",
        "    - Sequential (â€œYlGnâ€) â†’ for baseline & +1 Â°C maps.  \n",
        "    - Diverging (â€œBrBGâ€, â€œRdBuâ€, â€œcoolwarmâ€) â†’ for yield change map.  \n",
        "  - **Color limits:**  \n",
        "    Keep consistent `vmin/vmax` across maps for fair visual comparison.  \n",
        "  - **Styling:**  \n",
        "    - County borders: `edgecolor='grey', linewidth=0.2`  \n",
        "    - State borders: black, thicker lines for clarity.  \n",
        "    - Remove axes (`ax.axis(\"off\")`) and add descriptive titles.  \n",
        "  - **Interpretation:**  \n",
        "    - Brown/red â†’ yield decline (heat stress).  \n",
        "    - Green/blue â†’ yield increase or resilience.  \n",
        "\n",
        "- **Output:**  \n",
        "  Save high-resolution maps with legends and titles for consistent presentation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "abda9d0f",
      "metadata": {
        "id": "abda9d0f"
      },
      "outputs": [],
      "source": [
        "# Yield Difference Map (+1Â°C â€“ Baseline)\n",
        "\n",
        "fig, ax = plt.subplots(1, 1, figsize=(12, 8))\n",
        "\n",
        "counties_merged.plot(\n",
        "    column=\"pred_yield_plus1C_dif\",\n",
        "    cmap=\"BrBG\",\n",
        "    linewidth=0.2,\n",
        "    ax=ax,\n",
        "    edgecolor=\"grey\",\n",
        "    legend=True,\n",
        "    legend_kwds={'label': \"Yield change (bushels/acre)\"},\n",
        "    vmin = -40,\n",
        "    vmax = 40,\n",
        ")\n",
        "\n",
        "# overlay state borders with thicker lines\n",
        "states_with_data.boundary.plot(ax=ax, color=\"black\", linewidth=2)\n",
        "\n",
        "ax.set_title(\"County-level Predicted Yield Change (+1Â°C minus baseline)\", fontsize=14)\n",
        "ax.axis(\"off\")\n",
        "plt.savefig(\"yield_change_map.png\", dpi=300, bbox_inches=\"tight\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6b15ca6c",
      "metadata": {
        "id": "6b15ca6c"
      },
      "source": [
        "## **Part 2 â€” Classification: Recognizing Handwritten Digits (MNIST)**\n",
        "\n",
        "This section demonstrates a step-by-step workflow for **image classification** using the **MNIST dataset**, a classic benchmark for computer vision tasks.  \n",
        "You will learn how to preprocess image data, build a fully connected (Dense) neural network, and improve model performance through hyperparameter tuning with **KerasTuner**.\n",
        "\n",
        "**Workflow Overview**\n",
        "1. **Load and inspect** the MNIST dataset.  \n",
        "2. **Visualize** sample images and check class balance.  \n",
        "3. **Preprocess** the data for a dense neural network (flatten and normalize).  \n",
        "4. **Build, train, and evaluate** a baseline dense model.  \n",
        "5. **Tune hyperparameters** (layer size, activation, learning rate) using KerasTuner and retrain the best model.\n",
        "\n",
        "**Dataset Overview**\n",
        "- **Name:** MNIST (Modified National Institute of Standards and Technology).  \n",
        "- **Type:** 70,000 grayscale images of handwritten digits (0â€“9).  \n",
        "- **Dimensions:** Each image is **28 Ã— 28 pixels**.  \n",
        "- **Labels:** Integer digits from 0 to 9 (10 classes).  \n",
        "- **Split:**  \n",
        "  - 60,000 training samples  \n",
        "  - 10,000 testing samples\n",
        "\n",
        "## **2.1 Read the Data**\n",
        "\n",
        "ðŸ’¡ **Tips**\n",
        "\n",
        "- **Core Function:**  \n",
        "  Load the MNIST dataset and inspect its structure before preprocessing or modeling.  \n",
        "  This ensures that images and labels are correctly loaded, with the expected dimensions and value ranges.\n",
        "\n",
        "- **Key Checks:**\n",
        "  1. Confirm dataset dimensions (`.shape`) for both training and testing sets.  \n",
        "  2. Examine the **pixel value range** (should be 0â€“255 for grayscale images).  \n",
        "  3. Verify that **all 10 digit classes (0â€“9)** are present in the labels.  \n",
        "\n",
        "- **Useful Functions:**  \n",
        "  - `tf.keras.datasets.mnist.load_data()` â†’ loads pre-split training and test sets.  \n",
        "  - `.shape` â†’ displays the number of samples and image dimensions.  \n",
        "  - `.min()` / `.max()` â†’ checks pixel intensity range.  \n",
        "  - `np.unique()` â†’ lists unique label classes to verify class completeness."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "be8f8d2e",
      "metadata": {
        "id": "be8f8d2e"
      },
      "outputs": [],
      "source": [
        "# Load MNIST dataset from TensorFlow\n",
        "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "\n",
        "# Inspect dataset structure\n",
        "print(\"Training images shape:\", X_train.shape, \"labels shape:\", y_train.shape)\n",
        "print(\"Test images shape:    \", X_test.shape,  \"labels shape:\", y_test.shape)\n",
        "print(\"Pixel value range: min =\", X_train.min(), \"max =\", X_train.max())\n",
        "print(\"Unique class labels:\", np.unique(y_train))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5ae698ed",
      "metadata": {
        "id": "5ae698ed"
      },
      "source": [
        "## **2.2 Visualize Samples and Class Distribution**\n",
        "\n",
        "ðŸ’¡ **Tips**\n",
        "\n",
        "- **Core Function:**  \n",
        "  This step helps you **understand and validate the dataset** before training any model.  \n",
        "  Visualization ensures that the image data and labels are correctly paired, classes are balanced, and no anomalies exist.\n",
        "\n",
        "  Specifically, this section:\n",
        "  - Confirms the **imageâ€“label alignment** (each image matches its digit).  \n",
        "  - Checks **class balance** â€” verifying all digits (0â€“9) appear in similar proportions.  \n",
        "  - Builds early **intuition** about how digits are written and how clean the dataset is.\n",
        "\n",
        "- **Key Code Functions:**\n",
        "  - `plt.imshow()` â†’ displays grayscale images from the MNIST dataset.  \n",
        "  - `plt.subplot()` â†’ arranges multiple images in a grid layout.  \n",
        "  - `plt.hist()` â†’ plots a histogram showing how many samples exist for each class.  \n",
        "  - `np.arange(11) - 0.5` â†’ centers histogram bins on integer class labels (0â€“9).  \n",
        "  - `plt.tight_layout()` and `plt.suptitle()` â†’ improve figure spacing and readability."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "60e0d547",
      "metadata": {
        "id": "60e0d547"
      },
      "outputs": [],
      "source": [
        "# Function to visualize a grid of sample images with labels\n",
        "def show_samples(images, labels, n_rows=5, n_cols=5, title=\"MNIST samples\"):\n",
        "    plt.figure(figsize=(n_cols * 1.5, n_rows * 1.5))\n",
        "    plt.suptitle(title, fontsize=16)\n",
        "    for i in range(n_rows * n_cols):\n",
        "        ax = plt.subplot(n_rows, n_cols, i + 1)\n",
        "        plt.imshow(images[i], cmap=\"gray\")\n",
        "        plt.title(f\"Label: {labels[i]}\")\n",
        "        plt.axis(\"off\")\n",
        "    plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
        "    plt.show()\n",
        "\n",
        "# Display 25 example training images\n",
        "show_samples(X_train, y_train, n_rows=5, n_cols=5, title=\"First 25 Training Images\")\n",
        "\n",
        "# Visualize the number of samples per digit class\n",
        "plt.figure(figsize=(8, 4))\n",
        "plt.hist(y_train, bins=np.arange(11) - 0.5, rwidth=0.8, color=\"skyblue\", edgecolor=\"black\")\n",
        "plt.xticks(range(10))\n",
        "plt.xlabel(\"Digit Label\", fontsize=12)\n",
        "plt.ylabel(\"Count\", fontsize=12)\n",
        "plt.title(\"Training Set Class Distribution\", fontsize=14)\n",
        "plt.grid(axis=\"y\", alpha=0.3)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cc822b19",
      "metadata": {
        "id": "cc822b19"
      },
      "source": [
        "## **2.3 Build a Base Model**\n",
        "\n",
        "ðŸ’¡ **Tips**\n",
        "\n",
        "- **Core Function:**  \n",
        "  Prepare and train a **fully connected (Dense)** neural network on flattened MNIST images to create a **baseline classifier**.  \n",
        "  This model establishes a reference performance before applying hyperparameter tuning or more advanced architectures (e.g., convolutional neural networks).\n",
        "\n",
        "- **Key Steps:**\n",
        "\n",
        "  1. **Preprocess the data:**\n",
        "     - Flatten each 2D image (28Ã—28 pixels) into a **1D vector** of 784 features.  \n",
        "     - Normalize pixel values from **[0â€“255] â†’ [0â€“1]** for stable training and faster convergence.  \n",
        "       ```python\n",
        "       X_train_flat = X_train.reshape((-1, 28*28)).astype(\"float32\") / 255.0\n",
        "       ```\n",
        "\n",
        "  2. **Build a simple Dense network:**\n",
        "     - **Input:** 784 features (flattened pixels).  \n",
        "     - **Hidden Layer:** 16 neurons, ReLU activation.  \n",
        "     - **Output Layer:** 10 neurons (Softmax activation for classes 0â€“9).  \n",
        "       ```python\n",
        "       model = models.Sequential([\n",
        "           layers.Dense(16, activation='relu', input_shape=(784,)),\n",
        "           layers.Dense(10, activation='softmax')\n",
        "       ])\n",
        "       ```\n",
        "\n",
        "  3. **Compile the model:**\n",
        "     - **Optimizer:** `'adam'` â€” an adaptive learning rate algorithm that combines the benefits of RMSprop and momentum for stable, efficient convergence.  \n",
        "     - **Loss:** `'sparse_categorical_crossentropy'` â€” used when class labels are **integers (not one-hot encoded)**.  \n",
        "       - Unlike `'categorical_crossentropy'`, which expects labels in one-hot encoded form (e.g., `[0,0,0,1,0,0,0,0,0,0]`),  \n",
        "         `'sparse_categorical_crossentropy'` allows labels in simple integer form (e.g., `3` for the digit â€œ3â€).  \n",
        "       - This simplifies preprocessing and saves memory, especially for large datasets.  \n",
        "     - **Metric:** `'accuracy'` â€” measures the proportion of correctly classified images.\n",
        "\n",
        "  4. **Train the model:**\n",
        "     - Use **`batch_size=32`** to process 32 samples per gradient update (a common, balanced choice).  \n",
        "     - Reserve **10% of the training data** for validation (`validation_split=0.1`).  \n",
        "     - Add **EarlyStopping** to stop training automatically when validation loss stops improving:  \n",
        "       ```python\n",
        "       early_stop = EarlyStopping(\n",
        "           monitor='val_loss',\n",
        "           patience=3,\n",
        "           restore_best_weights=True\n",
        "       )\n",
        "       ```\n",
        "\n",
        "  5. **Evaluate model performance:**\n",
        "     - Compute accuracy on the test set to gauge baseline performance.  \n",
        "     - Typical accuracy for this small network is **~94â€“95%** on MNIST.\n",
        "\n",
        "  6. **Visualize predictions:**\n",
        "     - Display the first 15 test images with their true and predicted labels.  \n",
        "     - **Green titles** â†’ correct predictions; **Red titles** â†’ incorrect ones.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6c5d9712",
      "metadata": {
        "id": "6c5d9712"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# STEP 1 â€” Preprocess for Dense Neural Network\n",
        "# - Flatten 28x28 images â†’ 784-d vectors\n",
        "# - Normalize pixel values from [0,255] to [0,1]\n",
        "# ============================================================\n",
        "X_train_flat = X_train.reshape((-1, 28 * 28)).astype(\"float32\") / 255.0\n",
        "X_test_flat  = X_test.reshape((-1, 28 * 28)).astype(\"float32\") / 255.0\n",
        "\n",
        "print(\"After flattening: X_train_flat shape =\", X_train_flat.shape)\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# STEP 2 â€” Build a Simple Dense (Fully Connected) Model\n",
        "# ============================================================\n",
        "model = models.Sequential([\n",
        "    layers.Dense(16, activation='relu', input_shape=(784,)),  # hidden layer\n",
        "    layers.Dense(10, activation='softmax')                    # output layer (10 digits)\n",
        "])\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# STEP 3 â€” Compile the Model\n",
        "# ============================================================\n",
        "model.compile(\n",
        "    optimizer='adam',                             # adaptive optimizer\n",
        "    loss='sparse_categorical_crossentropy',       # suitable for integer labels\n",
        "    metrics=['accuracy']                          # track classification accuracy\n",
        ")\n",
        "\n",
        "# Display architecture summary\n",
        "model.summary()\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# STEP 4 â€” Train the Model (with EarlyStopping)\n",
        "# ============================================================\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "early_stop = EarlyStopping(\n",
        "    monitor='val_loss',          # monitor validation loss\n",
        "    patience=3,                  # stop if no improvement after 3 epochs\n",
        "    restore_best_weights=True,   # revert to best weights\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "history = model.fit(\n",
        "    X_train_flat, y_train,\n",
        "    epochs=50,                   # generous limit; EarlyStopping will stop earlier\n",
        "    batch_size=32,               # process 32 samples per update\n",
        "    validation_split=0.1,        # hold out 10% of training data for validation\n",
        "    callbacks=[early_stop],      # enable early stopping\n",
        "    verbose=2\n",
        ")\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# STEP 5 â€” Evaluate Model Performance on the Test Set\n",
        "# ============================================================\n",
        "test_loss, test_acc = model.evaluate(X_test_flat, y_test, verbose=0)\n",
        "print(f\"\\nTest accuracy: {test_acc:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "386d16f5",
      "metadata": {
        "id": "386d16f5"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# STEP 6 â€” Visualize Predictions (first 15 test samples)\n",
        "# - Green title = correct prediction; Red = incorrect\n",
        "# ============================================================\n",
        "\n",
        "# Generate predicted class probabilities for the first 15 test samples\n",
        "pred_probs = model.predict(X_test_flat[:15])\n",
        "\n",
        "# Convert probabilities (10 per sample) to the most likely class label (0â€“9)\n",
        "pred_labels = np.argmax(pred_probs, axis=1)\n",
        "\n",
        "# Create a visualization grid for comparison\n",
        "plt.figure(figsize=(12, 4))\n",
        "for i in range(15):\n",
        "    ax = plt.subplot(3, 5, i + 1)\n",
        "    plt.imshow(X_test[i], cmap=\"gray\")\n",
        "\n",
        "    # Color code predictions: green = correct, red = incorrect\n",
        "    color = \"green\" if pred_labels[i] == y_test[i] else \"red\"\n",
        "    plt.title(f\"True: {y_test[i]}  Pred: {pred_labels[i]}\", color=color, fontsize=10)\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "# Add a descriptive title for the figure\n",
        "plt.suptitle(\"Model Predictions on First 15 Test Images\", fontsize=14, weight=\"bold\")\n",
        "plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c6137a69",
      "metadata": {
        "id": "c6137a69"
      },
      "source": [
        "## **2.4 Hyperparameter Tuning**\n",
        "**Goal:**  \n",
        "  Build a tunable model structure that KerasTuner can repeatedly adjust and train using different hyperparameter combinations.  \n",
        "  The tuner evaluates each configuration based on **validation accuracy** and reports the best-performing setup.\n",
        "\n",
        "\n",
        "### **2.4.1 Define the Model Builder Function for Classification**\n",
        "\n",
        "ðŸ’¡ **Key Idea:**  \n",
        "The `build_model(hp)` function defines how the neural network is constructed using tunable parameters.  \n",
        "KerasTuner will repeatedly call this function during the search, substituting different hyperparameter values to find the optimal setup.\n",
        "\n",
        "- **Tunable parameters include:**\n",
        "  - `n_layers`: number of hidden layers to use (1â€“3).  \n",
        "  - `units`: number of neurons per hidden layer (32â€“256).  \n",
        "  - `activation`: activation function (`'relu'`, `'tanh'`, `'elu'`).  \n",
        "  - `learning_rate`: optimizer learning rate (`1e-2`, `1e-3`, `1e-4`).  \n",
        "\n",
        "- **Core function workflow:**\n",
        "  1. Define an input layer for flattened 28Ã—28 images.  \n",
        "  2. Dynamically add the number of Dense layers specified by `n_layers`, each with tuned `units` and `activation`.  \n",
        "  3. Add a 10-neuron output layer with **softmax** activation for multi-class classification.  \n",
        "  4. Compile the model with the **Adam** optimizer and the selected `learning_rate`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1a06ccbc",
      "metadata": {
        "id": "1a06ccbc"
      },
      "outputs": [],
      "source": [
        "def build_model(hp):\n",
        "    model = keras.Sequential()\n",
        "    model.add(layers.Input(shape=(784,)))  # input layer for flattened 28x28 images\n",
        "\n",
        "    # Tune number of hidden layers (1â€“3)\n",
        "    n_layers = hp.Int('n_layers', min_value=1, max_value=3)\n",
        "    for i in range(n_layers):\n",
        "        # Tune number of units and activation function for each layer\n",
        "        units = hp.Int(f'units_{i}', min_value=32, max_value=256, step=32)\n",
        "        activation = hp.Choice(f'activation_{i}', ['relu', 'tanh', 'elu'])\n",
        "        model.add(layers.Dense(units, activation=activation))\n",
        "\n",
        "    # Output layer (10 neurons for 10 digit classes)\n",
        "    model.add(layers.Dense(10, activation='softmax'))\n",
        "\n",
        "    # Tunable learning rate for Adam optimizer\n",
        "    lr = hp.Choice('learning_rate', [1e-2, 1e-3, 1e-4])\n",
        "    model.compile(\n",
        "        optimizer=keras.optimizers.Adam(learning_rate=lr),\n",
        "        loss='sparse_categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3d805db2",
      "metadata": {
        "id": "3d805db2"
      },
      "source": [
        "### **2.4.2 Initialize the Tuner (Random Search)**\n",
        "\n",
        "ðŸ’¡ **Tips**\n",
        "\n",
        "- **Core Function:**  \n",
        "  Set up a **KerasTuner RandomSearch** tuner to automatically test multiple combinations of hyperparameters defined in the `build_model(hp)` function.  \n",
        "  The tuner will evaluate each model configuration based on **validation accuracy**, rank the results, and store the best-performing models.\n",
        "\n",
        "- **Key Parameters:**\n",
        "  - **`build_model`** â†’ function defining the tunable model structure.  \n",
        "  - **`objective='val_accuracy'`** â†’ metric to maximize during tuning (classification objective).  \n",
        "  - **`max_trials=5`** â†’ number of different hyperparameter combinations to test.  \n",
        "  - **`executions_per_trial=1`** â†’ number of times each configuration is trained (higher values average out randomness but increase runtime).  \n",
        "  - **`directory` / `project_name`** â†’ specify where KerasTuner will store search logs and results.  \n",
        "  - **`seed`** â†’ ensures reproducibility of tuning results.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3e139899",
      "metadata": {
        "id": "3e139899"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# Initialize the Random Search Tuner\n",
        "# ============================================================\n",
        "tuner = kt.RandomSearch(\n",
        "    build_model,                     # model-building function\n",
        "    objective='val_accuracy',        # maximize validation accuracy\n",
        "    max_trials=5,                    # number of random combinations to test\n",
        "    executions_per_trial=1,          # train each configuration once\n",
        "    directory='kt_random_class',     # folder to store tuner results\n",
        "    project_name='mnist_class', # project name for this tuning task\n",
        "    seed=42                          # random seed for reproducibility\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f081704e",
      "metadata": {
        "id": "f081704e"
      },
      "source": [
        "### **2.4.3 Run the Tuner Search**\n",
        "\n",
        "ðŸ’¡ **Tips**\n",
        "\n",
        "- **Core Function:**  \n",
        "  Execute the **hyperparameter search** using the KerasTuner **RandomSearch** tuner.  \n",
        "  The tuner will repeatedly build, train, and evaluate models using different hyperparameter combinations defined in the `build_model(hp)` function.\n",
        "\n",
        "- **Key Concepts:**\n",
        "  - **EarlyStopping:**  \n",
        "    Prevents overfitting and saves time by stopping training early when the validation accuracy stops improving for several epochs.  \n",
        "    The best-performing model weights (based on validation accuracy) are automatically restored.\n",
        "  \n",
        "  - **Validation Split:**  \n",
        "    `validation_split=0.2` reserves 20% of the training data for validation during tuning â€” essential for the tuner to evaluate each configuration objectively.\n",
        "  \n",
        "  - **Epochs:**  \n",
        "    Set to 20 to give each trial enough training time. EarlyStopping will terminate training earlier if no improvement is observed.\n",
        "\n",
        "- **Key Parameters:**\n",
        "  - `monitor='val_accuracy'` â†’ tracks validation accuracy as the metric to optimize.  \n",
        "  - `patience=5` â†’ waits 5 epochs before stopping if no improvement.  \n",
        "  - `restore_best_weights=True` â†’ reloads the model weights from the best epoch.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e4090602",
      "metadata": {
        "id": "e4090602"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# Run the Tuner Search\n",
        "# ============================================================\n",
        "\n",
        "# Define EarlyStopping to prevent overfitting during tuning\n",
        "early_stop = keras.callbacks.EarlyStopping(\n",
        "    monitor='val_accuracy',       # metric to monitor\n",
        "    patience=5,                   # stop if no improvement after 5 epochs\n",
        "    mode='max',                   # maximize validation accuracy\n",
        "    restore_best_weights=True     # restore best weights after stopping\n",
        ")\n",
        "\n",
        "# Execute hyperparameter search\n",
        "tuner.search(\n",
        "    X_train_flat, y_train,\n",
        "    epochs=20,                    # upper limit; EarlyStopping may stop sooner\n",
        "    validation_split=0.2,         # reserve 20% for validation\n",
        "    callbacks=[early_stop],       # apply early stopping\n",
        "    verbose=2                     # print detailed tuning progress\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "24aaf686",
      "metadata": {
        "id": "24aaf686"
      },
      "source": [
        "### **2.4.4 Retrieve and Train the Best Model**\n",
        "\n",
        "ðŸ’¡ **Tips**\n",
        "\n",
        "- **Core Function:**  \n",
        "  Once the tuner completes the search, retrieve the **best hyperparameter configuration** found during tuning, rebuild the corresponding model, and train it more thoroughly to ensure optimal performance.\n",
        "\n",
        "\n",
        "- **Key Steps:**\n",
        "  1. **Retrieve best hyperparameters:**  \n",
        "     Use `tuner.get_best_hyperparameters(num_trials=1)[0]` to access the top-performing hyperparameter set.\n",
        "  2. **Inspect results:**  \n",
        "     Print the best hyperparameters to understand the optimal architecture chosen by KerasTuner.\n",
        "  3. **Rebuild the model:**  \n",
        "     Recreate the neural network using `tuner.hypermodel.build(best_hps)` with the tuned parameters.\n",
        "  4. **Train the final model:**  \n",
        "     Train the model again (e.g., for 30 epochs) using the same `validation_split` and `EarlyStopping` callback to ensure stable, high-accuracy performance.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "889f7bbb",
      "metadata": {
        "scrolled": true,
        "id": "889f7bbb"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# Retrieve and Train the Best Model\n",
        "# ============================================================\n",
        "\n",
        "# Retrieve the best hyperparameters from the tuning search\n",
        "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
        "print(\"\\nBest hyperparameters found:\")\n",
        "for k, v in best_hps.values.items():\n",
        "    print(f\"  {k}: {v}\")\n",
        "\n",
        "# Rebuild the model using the best hyperparameters\n",
        "best_model = tuner.hypermodel.build(best_hps)\n",
        "\n",
        "# Retrain the best model with EarlyStopping for stability\n",
        "history = best_model.fit(\n",
        "    X_train_flat, y_train,\n",
        "    epochs=30,                     # higher limit; EarlyStopping will halt early if needed\n",
        "    validation_split=0.2,          # 20% of training data used for validation\n",
        "    callbacks=[early_stop],        # stop when validation accuracy stops improving\n",
        "    verbose=2\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "829dbac7",
      "metadata": {
        "id": "829dbac7"
      },
      "source": [
        "### **2.4.5 Evaluate and Visualize**\n",
        "\n",
        "ðŸ’¡ **Tips**\n",
        "\n",
        "- **Core Function:**  \n",
        "  Evaluate the tuned modelâ€™s performance on unseen test data and visualize sample predictions to verify how accurately the model recognizes handwritten digits.\n",
        "\n",
        "- **Key Steps:**\n",
        "\n",
        "  1. **Evaluate model performance:**  \n",
        "     - Use the test dataset (`X_test_flat`, `y_test`) to compute the final **accuracy** and **loss**.  \n",
        "     - These metrics show how well the tuned model generalizes beyond the training data.\n",
        "     ```python\n",
        "     test_loss, test_acc = best_model.evaluate(X_test_flat, y_test, verbose=0)\n",
        "     ```\n",
        "\n",
        "  2. **Generate predictions:**  \n",
        "     - The model outputs **class probabilities** (confidence scores for each digit) when calling `predict()`.  \n",
        "       ```python\n",
        "       pred_probs = best_model.predict(X_test_flat[:15])\n",
        "       ```\n",
        "       Each row in `pred_probs` is a vector of 10 values (for digits 0â€“9) representing how confident the model is in each class.  \n",
        "     - To extract the most likely class for each sample, use:\n",
        "       ```python\n",
        "       pred_labels = np.argmax(pred_probs, axis=1)\n",
        "       ```\n",
        "       This picks the index (0â€“9) of the **maximum probability** in each row â€” giving the final **predicted digit label**.\n",
        "\n",
        "  3. **Visualize results:**  \n",
        "     - Display a few test images with their **True (T)** and **Predicted (P)** labels.  \n",
        "     - Use color cues to make results clear:  \n",
        "       - ðŸŸ© **Green title:** correct prediction.  \n",
        "       - ðŸŸ¥ **Red title:** incorrect prediction.  \n",
        "     - This visual check helps confirm that the model correctly identifies digit patterns and highlights any common misclassifications.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0681df51",
      "metadata": {
        "id": "0681df51"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# Evaluate and Visualize Model Performance\n",
        "# ============================================================\n",
        "\n",
        "# Evaluate tuned model on the test dataset\n",
        "test_loss, test_acc = best_model.evaluate(X_test_flat, y_test, verbose=0)\n",
        "print(f\"\\nTest accuracy: {test_acc:.4f}\")\n",
        "\n",
        "# Generate predictions for the first 15 test samples\n",
        "pred_probs = best_model.predict(X_test_flat[:15])\n",
        "pred_labels = np.argmax(pred_probs, axis=1)\n",
        "\n",
        "# Visualize true vs predicted labels\n",
        "plt.figure(figsize=(10, 4))\n",
        "for i in range(15):\n",
        "    ax = plt.subplot(3, 5, i + 1)\n",
        "    plt.imshow(X_test[i].reshape(28, 28), cmap=\"gray\")\n",
        "    color = \"green\" if pred_labels[i] == y_test[i] else \"red\"\n",
        "    plt.title(f\"T:{y_test[i]}  P:{pred_labels[i]}\", color=color, fontsize=9)\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "plt.suptitle(\"First 15 Test Images: True (T) vs Predicted (P)\", fontsize=14)\n",
        "plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
        "plt.show()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "25b5ce6f",
      "metadata": {
        "id": "25b5ce6f"
      },
      "source": [
        "# **Assignment 4: Neural Network Models**\n",
        "\n",
        "---\n",
        "\n",
        "### **Submission Instructions**\n",
        "- Begin with a **new Jupyter Notebook** containing your code, figures, and explanations **clearly separated by each question**.  \n",
        "- Save and submit your notebook using the filename format:  \n",
        "  **Lastname_Firstname_NetID_Assignment4.html**  \n",
        "- Label all figures and tables with **informative titles, axis labels, and legends**.  \n",
        "- Provide concise interpretations of your results and ensure all outputs are reproducible.\n",
        "\n",
        "---\n",
        "\n",
        "## **Part 1. Regression for Soybean Yield**  (4 pts)\n",
        "\n",
        "### **1.1 Scikit-learn Neural Network Regression (MLPRegressor)**  (2 pts)\n",
        "\n",
        "**Objective**  \n",
        "Develop both a baseline and tuned **feed-forward neural network** using scikit-learnâ€™s `MLPRegressor` to predict county-level soybean yields.  \n",
        "Evaluate how hyperparameter optimization affects model accuracy, stability, and generalization.\n",
        "\n",
        "**Tasks**  \n",
        "1. Load the dataset `US_soyb.csv` into a pandas DataFrame and inspect its structure.  \n",
        "2. Define predictor features (`X`) and the target variable (`y = ln_yield`).  \n",
        "3. Apply **one-hot encoding** to categorical features (e.g., `STATEFP`) to make the dataset numeric.  \n",
        "4. Split the data into **training (70%)** and **testing (30%)** subsets using `train_test_split()`.  \n",
        "5. Build and evaluate a **baseline model** using default parameters of `MLPRegressor`.  \n",
        "6. Use **RandomizedSearchCV** or **GridSearchCV** to tune the following hyperparameters:  \n",
        "   - `hidden_layer_sizes`, `activation`, `alpha`, and `learning_rate_init`.  \n",
        "7. Report:  \n",
        "   - The **best parameters**, **cross-validation RMSE**, and **RÂ²** score.  \n",
        "8. Evaluate the tuned model on the test set and compare it with the baseline model.  \n",
        "\n",
        "---\n",
        "\n",
        "### **1.2 TensorFlow/Keras Neural Network Regression (KerasTuner)**  (2 pts)\n",
        "\n",
        "**Objective**  \n",
        "Use TensorFlowâ€™s **Keras API** and **KerasTuner** to build, tune, and evaluate a neural network regression model for soybean yield prediction.  \n",
        "Explore how different architectures, activation functions, and learning rates affect predictive performance.\n",
        "\n",
        "**Tasks**  \n",
        "1. Prepare the dataset as in Section 1.1 (normalized features, trainâ€“test split).  \n",
        "2. Define a tunable **Sequential model** with:  \n",
        "   - Three fully connected (**Dense**) hidden layers,  \n",
        "   - Tunable hyperparameters for the number of neurons, activation functions, and learning rate.  \n",
        "3. Initialize a **KerasTuner RandomSearch** tuner and define an appropriate search space.  \n",
        "4. Implement an **EarlyStopping callback** that halts training when validation RMSE fails to improve for **10 consecutive epochs**, restoring the best weights.  \n",
        "5. Run the tuner search to identify the **best hyperparameter configuration** (lowest validation RMSE).  \n",
        "6. Rebuild and retrain the model using the best hyperparameters.  \n",
        "7. Evaluate the tuned model on the test set and report:  \n",
        "   - **RMSE** and **RÂ²** metrics,  \n",
        "   - An **Observed vs. Predicted** scatter plot (include a 1:1 reference line).  \n",
        "8. Summarize tuning improvements and discuss how to prevent **underfitting** or **overfitting**.  \n",
        "9. **Warming Scenario Analysis:**  \n",
        "   - Simulate a **+1Â°C mean temperature** scenario and predict yield responses.  \n",
        "   - Visualize the spatial distribution of yield changes.  \n",
        "   - Compare the spatial patterns of the results with your **tree-based models** (e.g., Random Forest, XGBoost) in Assignment 3 and interpret key similarities or differences.\n",
        "\n",
        "---\n",
        "\n",
        "## **Part 2. Classification for Handwritten Images (Fashion MNIST)**  (4 pts)\n",
        "\n",
        "### **2.1 Baseline Dense Neural Network**  (2 pts)\n",
        "\n",
        "**Objective**  \n",
        "Train a simple **dense neural network** using the **Fashion MNIST** dataset to classify images into ten clothing categories.  \n",
        "Establish baseline accuracy before tuning.\n",
        "\n",
        "**Tasks**  \n",
        "1. Load the **Fashion MNIST dataset** using `tf.keras.datasets.fashion_mnist.load_data()`.  \n",
        "2. Display the image dimensions (28 Ã— 28 pixels).  \n",
        "3. Visualize a grid of sample images with their corresponding labels.  \n",
        "4. Plot the class distribution to confirm all categories are well represented.  \n",
        "5. Preprocess the data:  \n",
        "   - Flatten each 28Ã—28 image into a **784-dimensional vector**,  \n",
        "   - Normalize pixel values to the **[0, 1] range** by dividing by 255.  \n",
        "6. Build a **Sequential model** with:  \n",
        "   - One hidden layer (choose the number of neurons and activation),  \n",
        "   - One output layer (Softmax activation for multi-class classification).  \n",
        "7. Compile the model using:  \n",
        "   - Optimizer: `adam`,  \n",
        "   - Loss: `sparse_categorical_crossentropy`,  \n",
        "   - Metric: `accuracy`.  \n",
        "8. Train the model with a **validation split of 0.1**, `batch_size=32`, and multiple epochs.  \n",
        "9. Report both **training** and **test accuracy**.  \n",
        "10. Visualize several test images, displaying **True (T)** vs **Predicted (P)** labels, color-coded (green = correct, red = incorrect).\n",
        "\n",
        "---\n",
        "\n",
        "### **2.2 Tuned Classification Model with KerasTuner**  (2 pts)\n",
        "\n",
        "**Objective**  \n",
        "Use **KerasTuner** to systematically optimize hyperparameters for the Fashion MNIST classification model and evaluate performance improvements over the baseline.\n",
        "\n",
        "**Tasks**  \n",
        "1. Define a tunable model function `build_model(hp)` that allows tuning of:  \n",
        "   - Number of hidden layers (1â€“3),  \n",
        "   - Neurons per layer (32â€“256),  \n",
        "   - Activation functions (`'relu'`, `'tanh'`, `'elu'`),  \n",
        "   - Learning rate (`1e-2`, `1e-3`, `1e-4`).  \n",
        "2. Initialize a **RandomSearch tuner** to maximize validation accuracy (`val_accuracy`).  \n",
        "3. Add an **EarlyStopping callback** to stop training when validation accuracy does not improve for **5 epochs**.  \n",
        "4. Run the tuner search and record the **best hyperparameter combination**.  \n",
        "5. Rebuild and retrain the model using the optimal hyperparameters.  \n",
        "6. Evaluate the tuned model on the test set and report:  \n",
        "   - **Accuracy** and **Loss**,  \n",
        "   - Performance comparison with the baseline model.  \n",
        "7. Visualize 15 test images showing **True (T)** vs **Predicted (P)** labels (green = correct, red = incorrect).  \n",
        "8. Discuss how the tuned model differs from the baseline.\n",
        "\n",
        "---\n",
        "\n",
        "## **Part 3. Summary of Insights**  (2 pt)\n",
        "\n",
        "**Objective**  \n",
        "Reflect on your experience training neural network models for both the **regression (soybean yield)** and **classification (Fashion MNIST)** tasks.  \n",
        "Summarize key takeaways about model architecture, data handling, and performance across different problem types.\n",
        "\n",
        "**Tasks**  \n",
        "1. Compare how neural networks address **regression** and **classification** tasks, focusing on differences in architecture design, loss function, and evaluation metrics.  \n",
        "2. Discuss how the **nature of the data** (tabular vs. image) affects model structure, preprocessing requirements, and learning complexity.  \n",
        "3. Highlight the main challenges encountered during training (e.g., convergence, overfitting) and how tuning or early stopping helped mitigate them.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1855fd9f",
      "metadata": {
        "id": "1855fd9f"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "geo_env",
      "language": "python",
      "name": "geo_env"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}